{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "484d333d-c5de-4c18-bf40-69b4fdab55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "67cb5e2e-6e00-4e09-8adb-30551b966e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        # optional trainable params\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # (batch,1)\n",
    "        # biased variance: divided by 1/(n-1)\n",
    "        # unbiased variance: divided by 1/n\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # (batch,1)\n",
    "        norm_x = (x-mean) / torch.sqrt(var+self.eps) # (batch,emb_dim)\n",
    "        return self.scale * norm_x + self.shift # (batch,emb_dim)\n",
    "        \n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh( torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3)) ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GeLU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out # 4\n",
    "        self.num_heads = num_heads # 2\n",
    "        self.head_dim = d_out // num_heads # 2\n",
    "        \n",
    "        # bigger weight matrices\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # compute big kqv matrices\n",
    "        keys = self.W_key(x) # (b,n,3)=>(b,n_token,4)\n",
    "        queries = self.W_query(x) # (b,n,3)=>(b,n_token,4)\n",
    "        values = self.W_value(x) # (b,n,3)=>(b,n_token,4)\n",
    "        # ... then splits\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head=2,head_dim=2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head,head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head,head_dim)\n",
    "        # swap <num_heads> to after-batch location\n",
    "        keys = keys.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "        queries = queries.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "        values = values.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3) # (b,n_head,n_token,head_dim)@(b,n_head,head_dim,n_token)=>(b,n_head,n_token,n_token)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # (n,n) with upper-right is -inf\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights) # (b,n_head,n_token,n_token)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2) # (b,n_head,n_token,head_dim=2)=>transpose to (b,n_token,n_head,head_dim=2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out) # (b,n_token,n_head*head_dim)=(b,n_token,4)\n",
    "        context_vec = self.out_proj(context_vec) # (b,n_token,4)\n",
    "\n",
    "        return context_vec # (b,n_token,4)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "        return x \n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # in_idx: (batch,seq_len), each element is a token-index (integer shows location)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx) # (batch,seq_len)=>(batch,seq_len,emb_dim)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) # (1,seq_len)=>(1,seq_len,emb_dim)\n",
    "        x = tok_embeds + pos_embeds # (batch,seq_len,emb_dim)\n",
    "        x = self.drop_emb(x) # (batch,seq_len,emb_dim)\n",
    "        x = self.trf_blocks(x) # (batch,seq_len,emb_dim)\n",
    "        x = self.final_norm(x) # (batch,seq_len,emb_dim)\n",
    "        logits = self.out_head(x) # (batch,seq_len,vocab_len)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25295c79-73a8-47c4-894b-03793d54d697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16033e-cc72-4ebf-ab78-7b9400b7a401",
   "metadata": {},
   "source": [
    "## Use GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dadd63c8-5b27-4e08-b780-b30beeb60b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx: (batch, n_tokens_long)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] # take last <context_size> tokens as context => (batch,context_size)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # (batch,context_size,vocab_len)\n",
    "        logits = logits[:,-1,:] # last tokenS - (batch,vocab_len)\n",
    "        probas = torch.softmax(logits,dim=-1) # (batch,vocab_len)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (batch,1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) #(batch,n_tokens_long+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4045be2c-8f05-4f8a-afc8-fc99152e7a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'}) # (n_tokens,)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # (1,n_tokens)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # (n_tokens,)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# token_ids: (batch, n_tokens+1)\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b9aa3-2659-42ab-baf4-bbc78e16fd2b",
   "metadata": {},
   "source": [
    "## Text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8feb04b7-0ea5-4010-be1d-d910e3d61f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833,3626,6100], # [\"every effort moves\"\n",
    "                       [40,1107,588]]) # \"I really like\"]\n",
    "targets = torch.tensor([[3626, 6100, 345], # [\"effort moves you\",\n",
    "                        [1107,588,11311]]) # \"really like chocolate\"]\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs) # (batch,n_tokens,len_vocab)=(2,3,50257)\n",
    "probas = torch.softmax(logits, dim=-1) # (batch,n_tokens,len_vocab)=(2,3,50257)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fbec11cc-a2e7-4c20-bbf9-8940764b9579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True) # (2,3,1)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "63b6dd40-27f6-43bc-b387-3a15459a2ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch1:  effort moves you\n",
      "Outputs batch1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Check output after convert back to text\n",
    "print(f\"Targets batch1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03c463d7-edcc-4fb3-9550-482d83185f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# 1st sentence: probs of the 3 correct words\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "# 2nd sentence: probs of the 3 correct words\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6951e35b-ff77-4e86-adc6-e5e8f2808667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "81d7f6ec-39d9-4ea2-89f0-a29a20883031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss:\n",
      " tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(\"Cross entropy loss:\\n\", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cf6871bd-87d8-49fc-883b-bbfbf5fc4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Pytorch cross-entropy:\n",
      " tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Pytorch will take ONE-HOT-LOGITS and INTEGER-TARGET\n",
    "logits_flat = logits.flatten(0,1) # (batch*n_tokens,vocab_len)\n",
    "targets_flat = targets.flatten() # (batch*n_tokens,)\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Pytorch cross-entropy:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab22396-e3e4-479a-8694-df3a070b8dc7",
   "metadata": {},
   "source": [
    "## Calculate train & val losses on real book i.e. \"The Verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e149177b-b2a0-40f4-8d40-5e778bae43f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20480\n",
      "Tokens: 5146\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8232d37e-a6e2-430f-b753-fa8b9ba2d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed my 'techniqu\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio*len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "print(train_data[-15:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12ff1051-d409-4ff2-9c77-9abcec455ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length] # (max_length,) - max_length is input dimension\n",
    "            target_chunk = token_ids[i+1:i+max_length+1] # (max_length,)\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        \n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eca513fe-c402-4ec2-9799-8d95b505106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data, batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6ed3c3f5-e048-4553-bf17-f43e974d4544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Target y is x shift one to the right\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6399dd2-f545-4cf0-a8cf-c362c1dc07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device) # (batch,n_token)\n",
    "    logits = model(input_batch) # (batch,n_token,embed_dim)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06e396aa-4427-4966-bab2-09751d7e9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a90b501-dd25-464c-8b2b-0a76e1bd486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.983159065246582\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "07201e4d-0e0f-4683-93a3-14eafce4061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c75708c9-bf2f-4b41-a155-7e00bdaf75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0] # 256\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device) # (batch,n_tokens)\n",
    "    with torch.no_grad():\n",
    "        # token_ids: (batch, n_tokens+50)\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef785ebe-600c-455f-9ae1-ed1fe0fb631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_iter: is NUM_BATCHES in evaluate (how many batches we want to evaluate)\n",
    "# start_context: sentence we evaluate the performance of model on\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] \n",
    "    # <tokens_seen>: every time we runs through a batch of (batch,n_tokens), we add up <batch*n_tokens>\n",
    "    # global_step: add up after every <batch_size> = 2 samples\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step() # update weights\n",
    "            # optional steps\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b950c67-6b82-4bd7-86d3-e8fb1f80cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 9.816, Val loss 9.924\n",
      "Epoch 1 (Step 000005): Train loss 8.069, Val loss 8.337\n",
      "Every effort moves you,.                                                \n",
      "Epoch 2 (Step 000010): Train loss 6.623, Val loss 7.051\n",
      "Epoch 2 (Step 000015): Train loss 6.045, Val loss 6.601\n",
      "Every effort moves you, and,, and,, and,,,, and,.                                   \n",
      "Epoch 3 (Step 000020): Train loss 5.528, Val loss 6.526\n",
      "Epoch 3 (Step 000025): Train loss 5.382, Val loss 6.392\n",
      "Every effort moves you.                                                 \n",
      "Epoch 4 (Step 000030): Train loss 4.865, Val loss 6.260\n",
      "Epoch 4 (Step 000035): Train loss 4.635, Val loss 6.297\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the the picture.              \n",
      "Epoch 5 (Step 000040): Train loss 3.943, Val loss 6.181\n",
      "Every effort moves you know the                                                \n",
      "Epoch 6 (Step 000045): Train loss 3.552, Val loss 6.177\n",
      "Epoch 6 (Step 000050): Train loss 2.976, Val loss 6.133\n",
      "Every effort moves you know it was not that the picture.  \"I had the last word.           \"I turned back his head to the donkey.  \"I looked, and I had a little his\n",
      "Epoch 7 (Step 000055): Train loss 2.859, Val loss 6.166\n",
      "Epoch 7 (Step 000060): Train loss 2.139, Val loss 6.148\n",
      "Every effort moves you know,\" was not that my dear, and he had been the picture was a so that he was a little to me to have to see a smile behind his pictures.      \"I looked, and were, and I was\n",
      "Epoch 8 (Step 000065): Train loss 1.699, Val loss 6.187\n",
      "Epoch 8 (Step 000070): Train loss 1.400, Val loss 6.233\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 9 (Step 000075): Train loss 1.071, Val loss 6.282\n",
      "Epoch 9 (Step 000080): Train loss 0.808, Val loss 6.305\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--because he's the first\n",
      "Epoch 10 (Step 000085): Train loss 0.588, Val loss 6.409\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, \n",
    "    eval_freq=5, eval_iter=5, start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec52ee1-56b6-4762-b2ff-ec1a6131c37f",
   "metadata": {},
   "source": [
    "### View LLM train and val losses curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bec2fac0-4658-4ecb-9cb9-230641dc6036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # 2nd x-axis that shares same y-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73b45e-e178-4b0c-9b8f-ab8f5ea4eff8",
   "metadata": {},
   "source": [
    "## Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a14eab60-9b8c-4a2a-8bd3-786f8db4b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "# token_ids: (batch, n_tokens+50)\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e74e77-30c0-4fdf-9a23-7cafc09cdbbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
