{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "484d333d-c5de-4c18-bf40-69b4fdab55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,\n",
    "    \"context_length\": 256,\n",
    "    \"emb_dim\": 768,\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67cb5e2e-6e00-4e09-8adb-30551b966e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        # optional trainable params\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # (batch,1)\n",
    "        # biased variance: divided by 1/(n-1)\n",
    "        # unbiased variance: divided by 1/n\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # (batch,1)\n",
    "        norm_x = (x-mean) / torch.sqrt(var+self.eps) # (batch,emb_dim)\n",
    "        return self.scale * norm_x + self.shift # (batch,emb_dim)\n",
    "        \n",
    "\n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh( torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3)) ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "            GeLU(),\n",
    "            nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out # 4\n",
    "        self.num_heads = num_heads # 2\n",
    "        self.head_dim = d_out // num_heads # 2\n",
    "        \n",
    "        # bigger weight matrices\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        \n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "        \n",
    "        # compute big kqv matrices\n",
    "        keys = self.W_key(x) # (b,n,3)=>(b,n_token,4)\n",
    "        queries = self.W_query(x) # (b,n,3)=>(b,n_token,4)\n",
    "        values = self.W_value(x) # (b,n,3)=>(b,n_token,4)\n",
    "        # ... then splits\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head=2,head_dim=2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head,head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head,head_dim)\n",
    "        # swap <num_heads> to after-batch location\n",
    "        keys = keys.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "        queries = queries.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "        values = values.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3) # (b,n_head,n_token,head_dim)@(b,n_head,head_dim,n_token)=>(b,n_head,n_token,n_token)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # (n,n) with upper-right is -inf\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights) # (b,n_head,n_token,n_token)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2) # (b,n_head,n_token,head_dim=2)=>transpose to (b,n_token,n_head,head_dim=2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out) # (b,n_token,n_head*head_dim)=(b,n_token,4)\n",
    "        context_vec = self.out_proj(context_vec) # (b,n_token,4)\n",
    "\n",
    "        return context_vec # (b,n_token,4)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "        return x \n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # in_idx: (batch,seq_len), each element is a token-index (integer shows location)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx) # (batch,seq_len)=>(batch,seq_len,emb_dim)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) # (1,seq_len)=>(1,seq_len,emb_dim)\n",
    "        x = tok_embeds + pos_embeds # (batch,seq_len,emb_dim)\n",
    "        x = self.drop_emb(x) # (batch,seq_len,emb_dim)\n",
    "        x = self.trf_blocks(x) # (batch,seq_len,emb_dim)\n",
    "        x = self.final_norm(x) # (batch,seq_len,emb_dim)\n",
    "        logits = self.out_head(x) # (batch,seq_len,vocab_len)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25295c79-73a8-47c4-894b-03793d54d697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16033e-cc72-4ebf-ab78-7b9400b7a401",
   "metadata": {},
   "source": [
    "## Use GPT to generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dadd63c8-5b27-4e08-b780-b30beeb60b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx: (batch, n_tokens_long)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] # take last <context_size> tokens as context => (batch,context_size)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # (batch,context_size,vocab_len)\n",
    "        logits = logits[:,-1,:] # last tokenS - (batch,vocab_len)\n",
    "        probas = torch.softmax(logits,dim=-1) # (batch,vocab_len)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (batch,1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) #(batch,n_tokens_long+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4045be2c-8f05-4f8a-afc8-fc99152e7a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you rentingetic wasnÙ… refres RexMeCHicular stren\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'}) # (n_tokens,)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # (1,n_tokens)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # (n_tokens,)\n",
    "    return tokenizer.decode(flat.tolist())\n",
    "\n",
    "start_context = \"Every effort moves you\"\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "# token_ids: (batch, n_tokens+1)\n",
    "token_ids = generate_text_simple(\n",
    "    model = model,\n",
    "    idx = text_to_token_ids(start_context, tokenizer),\n",
    "    max_new_tokens = 10,\n",
    "    context_size = GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b9aa3-2659-42ab-baf4-bbc78e16fd2b",
   "metadata": {},
   "source": [
    "## Text generation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8feb04b7-0ea5-4010-be1d-d910e3d61f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 50257])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.tensor([[16833,3626,6100], # [\"every effort moves\"\n",
    "                       [40,1107,588]]) # \"I really like\"]\n",
    "targets = torch.tensor([[3626, 6100, 345], # [\"effort moves you\",\n",
    "                        [1107,588,11311]]) # \"really like chocolate\"]\n",
    "with torch.no_grad():\n",
    "    logits = model(inputs) # (batch,n_tokens,len_vocab)=(2,3,50257)\n",
    "probas = torch.softmax(logits, dim=-1) # (batch,n_tokens,len_vocab)=(2,3,50257)\n",
    "print(probas.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbec11cc-a2e7-4c20-bbf9-8940764b9579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token IDs:\n",
      " tensor([[[16657],\n",
      "         [  339],\n",
      "         [42826]],\n",
      "\n",
      "        [[49906],\n",
      "         [29669],\n",
      "         [41751]]])\n"
     ]
    }
   ],
   "source": [
    "token_ids = torch.argmax(probas, dim=-1, keepdim=True) # (2,3,1)\n",
    "print(\"Token IDs:\\n\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b6dd40-27f6-43bc-b387-3a15459a2ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets batch1:  effort moves you\n",
      "Outputs batch1:  Armed heNetflix\n"
     ]
    }
   ],
   "source": [
    "# Check output after convert back to text\n",
    "print(f\"Targets batch1: {token_ids_to_text(targets[0], tokenizer)}\")\n",
    "print(f\"Outputs batch1: {token_ids_to_text(token_ids[0].flatten(), tokenizer)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03c463d7-edcc-4fb3-9550-482d83185f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 1: tensor([7.4540e-05, 3.1061e-05, 1.1563e-05])\n",
      "Text 2: tensor([1.0337e-05, 5.6776e-05, 4.7559e-06])\n"
     ]
    }
   ],
   "source": [
    "# 1st sentence: probs of the 3 correct words\n",
    "text_idx = 0\n",
    "target_probas_1 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 1:\", target_probas_1)\n",
    "# 2nd sentence: probs of the 3 correct words\n",
    "text_idx = 1\n",
    "target_probas_2 = probas[text_idx, [0,1,2], targets[text_idx]]\n",
    "print(\"Text 2:\", target_probas_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6951e35b-ff77-4e86-adc6-e5e8f2808667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -9.5042, -10.3796, -11.3677, -11.4798,  -9.7764, -12.2561])\n"
     ]
    }
   ],
   "source": [
    "log_probas = torch.log(torch.cat((target_probas_1, target_probas_2)))\n",
    "print(log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81d7f6ec-39d9-4ea2-89f0-a29a20883031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss:\n",
      " tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "avg_log_probas = torch.mean(log_probas)\n",
    "neg_avg_log_probas = avg_log_probas * -1\n",
    "print(\"Cross entropy loss:\\n\", neg_avg_log_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf6871bd-87d8-49fc-883b-bbfbf5fc4944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened logits: torch.Size([6, 50257])\n",
      "Flattened targets: torch.Size([6])\n",
      "Pytorch cross-entropy:\n",
      " tensor(10.7940)\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Pytorch will take ONE-HOT-LOGITS and INTEGER-TARGET\n",
    "logits_flat = logits.flatten(0,1) # (batch*n_tokens,vocab_len)\n",
    "targets_flat = targets.flatten() # (batch*n_tokens,)\n",
    "print(\"Flattened logits:\", logits_flat.shape)\n",
    "print(\"Flattened targets:\", targets_flat.shape)\n",
    "loss = torch.nn.functional.cross_entropy(logits_flat, targets_flat)\n",
    "print(\"Pytorch cross-entropy:\\n\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab22396-e3e4-479a-8694-df3a070b8dc7",
   "metadata": {},
   "source": [
    "## Calculate train & val losses on real book i.e. \"The Verdict\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e149177b-b2a0-40f4-8d40-5e778bae43f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Characters: 20480\n",
      "Tokens: 5146\n"
     ]
    }
   ],
   "source": [
    "file_path = \"the-verdict.txt\"\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    text_data = file.read()\n",
    "total_characters = len(text_data)\n",
    "total_tokens = len(tokenizer.encode(text_data))\n",
    "print(\"Characters:\", total_characters)\n",
    "print(\"Tokens:\", total_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8232d37e-a6e2-430f-b753-fa8b9ba2d60e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ed my 'techniqu\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.90\n",
    "split_idx = int(train_ratio*len(text_data))\n",
    "train_data = text_data[:split_idx]\n",
    "val_data = text_data[split_idx:]\n",
    "print(train_data[-15:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12ff1051-d409-4ff2-9c77-9abcec455ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        for i in range(0, len(token_ids)-max_length, stride):\n",
    "            input_chunk = token_ids[i:i+max_length] # (max_length,) - max_length is input dimension\n",
    "            target_chunk = token_ids[i+1:i+max_length+1] # (max_length,)\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "        \n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "    return dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eca513fe-c402-4ec2-9799-8d95b505106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "train_loader = create_dataloader_v1(\n",
    "    train_data, batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=True, \n",
    "    shuffle=True,\n",
    "    num_workers=0)\n",
    "val_loader = create_dataloader_v1(\n",
    "    val_data,\n",
    "    batch_size=2,\n",
    "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "    drop_last=False,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6ed3c3f5-e048-4553-bf17-f43e974d4544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n",
      "\n",
      "Validation loader:\n",
      "torch.Size([2, 256]) torch.Size([2, 256])\n"
     ]
    }
   ],
   "source": [
    "# Target y is x shift one to the right\n",
    "print(\"Train loader:\")\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)\n",
    "print(\"\\nValidation loader:\")\n",
    "for x, y in val_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c6399dd2-f545-4cf0-a8cf-c362c1dc07e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device) # (batch,n_token)\n",
    "    logits = model(input_batch) # (batch,n_token,embed_dim)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06e396aa-4427-4966-bab2-09751d7e9224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a90b501-dd25-464c-8b2b-0a76e1bd486a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU1 NVIDIA GeForce GT 1030 which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GT 1030 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GT 1030 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 10.98758347829183\n",
      "Validation loss: 10.983159065246582\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(train_loader, model, device)\n",
    "    val_loss = calc_loss_loader(val_loader, model, device)\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "07201e4d-0e0f-4683-93a3-14eafce4061a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c75708c9-bf2f-4b41-a155-7e00bdaf75bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0] # 256\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device) # (batch,n_tokens)\n",
    "    with torch.no_grad():\n",
    "        # token_ids: (batch, n_tokens+50)\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ef785ebe-600c-455f-9ae1-ed1fe0fb631e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_iter: is NUM_BATCHES in evaluate (how many batches we want to evaluate)\n",
    "# start_context: sentence we evaluate the performance of model on\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] \n",
    "    # <tokens_seen>: every time we runs through a batch of (batch,n_tokens), we add up <batch*n_tokens>\n",
    "    # global_step: add up after every <batch_size> = 2 samples\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step() # update weights\n",
    "            # optional steps\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b950c67-6b82-4bd7-86d3-e8fb1f80cdc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 9.816, Val loss 9.924\n",
      "Epoch 1 (Step 000005): Train loss 8.069, Val loss 8.337\n",
      "Every effort moves you,.                                                \n",
      "Epoch 2 (Step 000010): Train loss 6.623, Val loss 7.051\n",
      "Epoch 2 (Step 000015): Train loss 6.045, Val loss 6.601\n",
      "Every effort moves you, and,, and,, and,,,, and,.                                   \n",
      "Epoch 3 (Step 000020): Train loss 5.528, Val loss 6.526\n",
      "Epoch 3 (Step 000025): Train loss 5.382, Val loss 6.392\n",
      "Every effort moves you.                                                 \n",
      "Epoch 4 (Step 000030): Train loss 4.865, Val loss 6.260\n",
      "Epoch 4 (Step 000035): Train loss 4.635, Val loss 6.297\n",
      "Every effort moves you of the \"I the picture.                    \"I\"I the the picture.              \n",
      "Epoch 5 (Step 000040): Train loss 3.943, Val loss 6.181\n",
      "Every effort moves you know the                                                \n",
      "Epoch 6 (Step 000045): Train loss 3.552, Val loss 6.177\n",
      "Epoch 6 (Step 000050): Train loss 2.976, Val loss 6.133\n",
      "Every effort moves you know it was not that the picture.  \"I had the last word.           \"I turned back his head to the donkey.  \"I looked, and I had a little his\n",
      "Epoch 7 (Step 000055): Train loss 2.859, Val loss 6.166\n",
      "Epoch 7 (Step 000060): Train loss 2.139, Val loss 6.148\n",
      "Every effort moves you know,\" was not that my dear, and he had been the picture was a so that he was a little to me to have to see a smile behind his pictures.      \"I looked, and were, and I was\n",
      "Epoch 8 (Step 000065): Train loss 1.699, Val loss 6.187\n",
      "Epoch 8 (Step 000070): Train loss 1.400, Val loss 6.233\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
      "Epoch 9 (Step 000075): Train loss 1.071, Val loss 6.282\n",
      "Epoch 9 (Step 000080): Train loss 0.808, Val loss 6.305\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the honour being _mine_--because he's the first\n",
      "Epoch 10 (Step 000085): Train loss 0.588, Val loss 6.409\n",
      "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.0004, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 10\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device, num_epochs=num_epochs, \n",
    "    eval_freq=5, eval_iter=5, start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec52ee1-56b6-4762-b2ff-ec1a6131c37f",
   "metadata": {},
   "source": [
    "### View LLM train and val losses curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bec2fac0-4658-4ecb-9cb9-230641dc6036",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # 2nd x-axis that shares same y-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c73b45e-e178-4b0c-9b8f-ab8f5ea4eff8",
   "metadata": {},
   "source": [
    "## Decoding strategies to control randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a14eab60-9b8c-4a2a-8bd3-786f8db4b3f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output text:\n",
      " Every effort moves you?\"\n",
      "\n",
      "\"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.to(\"cpu\")\n",
    "model.eval()\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "token_ids = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=25,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
    ")\n",
    "# token_ids: (batch, n_tokens+50)\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308e69ad-b524-4b98-8bb9-5c493b66536d",
   "metadata": {},
   "source": [
    "### Temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "310857bb-8b41-4345-b19f-e5406e5fd3f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward\n"
     ]
    }
   ],
   "source": [
    "# Redo original approach \n",
    "vocab = {\n",
    "    \"closer\": 0,\n",
    "    \"every\": 1,\n",
    "    \"effort\": 2,\n",
    "    \"forward\": 3,\n",
    "    \"inches\": 4,\n",
    "    \"moves\": 5,\n",
    "    \"pizza\": 6,\n",
    "    \"toward\": 7,\n",
    "    \"you\": 8 \n",
    "}\n",
    "inverse_vocab = {v:k for k,v in vocab.items()}\n",
    "next_token_logits = torch.tensor([4.51, 0.89, -1.90, 6.75, 1.63, -1.62, -1.89, 6.28, 1.79])\n",
    "probas = torch.softmax(next_token_logits, dim=0) # (vocab_len,)\n",
    "next_token_id = torch.argmax(probas).item()\n",
    "print(inverse_vocab[next_token_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3ddc5487-ef34-41ca-8862-55723b04cbd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id: 3\n",
      "73 x closer\n",
      "0 x every\n",
      "0 x effort\n",
      "581 x forward\n",
      "2 x inches\n",
      "0 x moves\n",
      "0 x pizza\n",
      "344 x toward\n"
     ]
    }
   ],
   "source": [
    "# Replace argmax with sample from a probability distribution => TORCH.MULTINOMIAL\n",
    "torch.manual_seed(123)\n",
    "next_token_id = torch.multinomial(probas, num_samples=1).item()\n",
    "print(\"Next token id:\", next_token_id)\n",
    "# Select that word 1000 times\n",
    "def print_sampled_tokens(probas):\n",
    "    sample = [torch.multinomial(probas, num_samples=1).item()\n",
    "              for i in range(1000)]\n",
    "    sampled_ids = torch.bincount(torch.tensor(sample)) # [<num_items_0>, <num_items_1>, ...]\n",
    "    for i, freq  in enumerate(sampled_ids):\n",
    "        print(f\"{freq} x {inverse_vocab[i]}\")\n",
    "print_sampled_tokens(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1b0c4c8-3d40-4aaf-bc70-4a0a582d27d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Temperature scaling\n",
    "def softmax_with_temperature(logits, temperature):\n",
    "    scaled_logits = logits/temperature\n",
    "    return torch.softmax(scaled_logits, dim=0)\n",
    "temperatures = [1, 0.1, 5]\n",
    "# <next_token_logits>: (vocab_len,)\n",
    "scaled_probas = [softmax_with_temperature(next_token_logits, T) for T in temperatures] # list of (vocab_len,) each\n",
    "x = torch.arange(len(vocab))\n",
    "bar_width = 0.15\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "for i, T in enumerate(temperatures):\n",
    "    rects = ax.bar(x+i*bar_width, scaled_probas[i], bar_width, label=f\"Temperature={T}\")\n",
    "ax.set_ylabel(\"Probability\")\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vocab.keys(), rotation=90)\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c56825-da78-49f6-a59a-ee70c4bac17a",
   "metadata": {},
   "source": [
    "## Top-k sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "784e2968-6235-49a3-9449-3c9d6c44e241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top logits: tensor([6.7500, 6.2800, 4.5100])\n",
      "Top positions: tensor([3, 7, 0])\n"
     ]
    }
   ],
   "source": [
    "top_k = 3\n",
    "top_logits, top_pos = torch.topk(next_token_logits, top_k) # (3,) and (3,)\n",
    "print(\"Top logits:\", top_logits)\n",
    "print(\"Top positions:\", top_pos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "34e0887a-f72d-41bb-96a7-5c1f14989be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4.5100,   -inf,   -inf, 6.7500,   -inf,   -inf,   -inf, 6.2800,   -inf])\n"
     ]
    }
   ],
   "source": [
    "# Set logits outside top-3 to -inf\n",
    "new_logits = torch.where(\n",
    "    condition = next_token_logits < top_logits[-1],\n",
    "    input = torch.tensor(float('-inf')),\n",
    "    other = next_token_logits\n",
    ")\n",
    "print(new_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cc4f7f65-64a1-4373-b6b5-19142e376681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0615, 0.0000, 0.0000, 0.5775, 0.0000, 0.0000, 0.0000, 0.3610, 0.0000])\n"
     ]
    }
   ],
   "source": [
    "# probs of all entries in <vocab> (for the next token)\n",
    "topk_probas = torch.softmax(new_logits, dim=0)\n",
    "print(topk_probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "905922bf-696c-4ddc-a25b-1f93c24f8696",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text generation with temperature scaling & top-k & multinomial-sampling\n",
    "# idx: (batch,seq_len) with batch=1 for now\n",
    "# eos_id: stop generating when we generate the char <eos_id>\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # crop if sentence too long\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # (1,seq_len,vocab_len)\n",
    "        # take last token in predicted sequence\n",
    "        logits = logits[:,-1,:] # (1,vocab_len)\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            # change outside-top3 logits to \"-inf\"\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        # temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits/temperature # (1,vocab_len)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else: \n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) # (1,1)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch,seq_len+1)\n",
    "    return idx # (batch, seq_len+num_new_tokens)\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "87faa82f-b163-4383-831d-f8a3bef78b5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "Output text:\n",
      " Every effort moves you can,\" was one of the, one of lo that point across the enough\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx = text_to_token_ids(\"Every effort moves you\", tokenizer),\n",
    "    max_new_tokens=15,\n",
    "    context_size=GPT_CONFIG_124M[\"context_length\"],\n",
    "    top_k=25,\n",
    "    temperature=1.4\n",
    ")\n",
    "print(token_ids.shape)\n",
    "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651eaf5-6a31-4cb4-8833-8119e85f25ea",
   "metadata": {},
   "source": [
    "## Loading and saving model weights in Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce1172ef-9850-4efd-81bf-5fe6e5bb8615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b36f90bd-eec4-4ea2-bf93-80f53186ad6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b12ecd0-001d-4af5-a5b7-ad4b93e122d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save both model weights and optimizer\n",
    "torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "    }, \"model_and_optimizer.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "201af5a3-bfbd-403e-863e-418c676a06ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 768)\n",
       "  (pos_emb): Embedding(256, 768)\n",
       "  (drop_emb): Dropout(p=0.1, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_key): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (W_value): Linear(in_features=768, out_features=768, bias=False)\n",
       "        (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load both model weights and optimizer\n",
    "checkpoint = torch.load(\"model_and_optimizer.pth\", map_location=device)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=0.1)\n",
    "optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55a427d-bcb8-4365-b8d6-64327309c30f",
   "metadata": {},
   "source": [
    "## Loading pretrained weights from OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcd2cb-48ea-4b52-8bb2-8060687493a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
