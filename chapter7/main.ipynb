{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8ee7f5-1087-41a7-9f98-eb745cde39c2",
   "metadata": {},
   "source": [
    "# Fine-tuning to follow instructions\n",
    "## Prepare dataset for supervised instruction fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68b6393c-2648-409c-a8fb-a566b95b3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import os, json, urllib, torch\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    # compose .json file\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    # read .json file to get <data>\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "        \n",
    "file_path = \"instruction-data.json\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19428ba1-91ab-4111-bb4d-301097bddf2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# Print 1 entry to test \n",
    "# => {instruction:\"\", input:\"\", output:\"\"}\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c10774-9ae4-4a97-ad4b-a3b976dff6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01616c-5199-4e42-b2fd-3fe178d52fb1",
   "metadata": {},
   "source": [
    "### Convert \"data\" to Alpaca-style prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2051835-a25c-459c-a767-b4fc412cf106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text\n",
    "\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90dd4fba-99d7-4c30-a151-7b8cd3cdb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# In this case the field \"input\" is missing\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d6c14-2443-413e-95ed-f24f61684798",
   "metadata": {},
   "source": [
    "### Partioning the data in to train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a565a413-008e-4891-9ac1-a4dc15997f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data)*0.85)\n",
    "test_portion = int(len(data)*0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion+test_portion]\n",
    "val_data = data[train_portion+test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data)) # 85%\n",
    "print(\"Validation set length:\", len(val_data)) # 5%\n",
    "print(\"Test set length:\", len(test_data)) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafe828-e3c7-45a3-8a21-519021836f4b",
   "metadata": {},
   "source": [
    "## Organizing data into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2672b-42a7-419a-8e0e-055d3b8d7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    # data: list of {'instruction': 'Identify...', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data \n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # list of e.g [23,1,244,...]/[2,456,..]/; each is encoded conversation text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "779f903f-7d80-4e18-9463-ab94ba5e5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Check id of the padding <|endoftext|> token\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac1ef53b-85f4-4efc-94e7-977e59d2e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Padding to the longest length IN THE BATCH ONLY\n",
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch) # max length of each prompt PLUS 1\n",
    "    \n",
    "    inputs_lst = []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = new_item + [pad_token_id]*(batch_max_length-len(new_item)) # even the longest prompt with be padded with <50256>\n",
    "        inputs = torch.tensor(padded[:-1]) # last <50256> token removed => tensor of (max_len,)\n",
    "        inputs_lst.append(inputs)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device) # (batch,max_len)\n",
    "    return inputs_tensor\n",
    "\n",
    "inputs_1 = [0,1,2,3,4]\n",
    "inputs_2 = [5,6]\n",
    "inputs_3 = [7,8,9]\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e218fdb2-46ce-421b-9b43-997e2b07592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Create TARGET TOKEN IDs\n",
    "# which is input-token-ids shifted by 1 to the left; then a PADDING-TOKEN at the end\n",
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch) # max length of each prompt PLUS 1\n",
    "    \n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = new_item + [pad_token_id]*(batch_max_length-len(new_item)) # even the longest prompt will be padded with <50256>\n",
    "        inputs = torch.tensor(padded[:-1]) # last <50256> token removed => tensor of (max_len,)\n",
    "        targets = torch.tensor(padded[1:]) # old sentence shifted left, then an additional <50256> on the right => tensor of (max_len,)\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device) # (batch,max_len)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device) # (batch,max_len)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e27b95a4-10a8-449d-9688-8925b400fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Replace padding tokens with <-100> tokens (EXCEPT the EndOfLine)\n",
    "# batch_max_length: length of the longest sentence in that batch\n",
    "# allowed_max_length: if a sentence is longer than e.g. 1024 tokens => trim it down\n",
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch) # max length of each sentence PLUS 1\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = new_item + [pad_token_id]*(batch_max_length-len(new_item)) # even the longest prompt will be padded with <50256>\n",
    "        inputs = torch.tensor(padded[:-1]) # last <50256> token removed => tensor of (max_len,)\n",
    "        targets = torch.tensor(padded[1:]) # old sentence shifted left, then an additional <50256> on the right => tensor of (max_len,)\n",
    "\n",
    "        mask = targets == pad_token_id # (max_len,) of bools; True where <50256>s are\n",
    "        # location of nonzero locations (num_nonzeros,1) => squeeze to (num_nonzeros,); each is index location\n",
    "        indices = torch.nonzero(mask).squeeze() \n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index # change every last indices to -100 except one\n",
    "\n",
    "        # trim down too long sentences\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs) # (batch,max_len)\n",
    "        targets_lst.append(targets) # (batch,max_len)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0aecb5f-958d-4d76-b138-2e5c03bb7a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
