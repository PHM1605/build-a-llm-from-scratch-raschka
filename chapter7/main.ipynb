{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f8ee7f5-1087-41a7-9f98-eb745cde39c2",
   "metadata": {},
   "source": [
    "# Fine-tuning to follow instructions\n",
    "## Prepare dataset for supervised instruction fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68b6393c-2648-409c-a8fb-a566b95b3267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entries: 1100\n"
     ]
    }
   ],
   "source": [
    "import os, json, urllib, torch\n",
    "\n",
    "def download_and_load_file(file_path, url):\n",
    "    # compose .json file\n",
    "    if not os.path.exists(file_path):\n",
    "        with urllib.request.urlopen(url) as response:\n",
    "            text_data = response.read().decode(\"utf-8\")\n",
    "        with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(text_data)\n",
    "    else:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            text_data = file.read()\n",
    "    # read .json file to get <data>\n",
    "    with open(file_path, \"r\") as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "        \n",
    "file_path = \"instruction-data.json\"\n",
    "url = \"https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/main/ch07/01_main-chapter-code/instruction-data.json\"\n",
    "data = download_and_load_file(file_path, url)\n",
    "print(\"Number of entries:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19428ba1-91ab-4111-bb4d-301097bddf2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example entry:\n",
      " {'instruction': 'Identify the correct spelling of the following word.', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n"
     ]
    }
   ],
   "source": [
    "# Print 1 entry to test \n",
    "# => {instruction:\"\", input:\"\", output:\"\"}\n",
    "print(\"Example entry:\\n\", data[50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4c10774-9ae4-4a97-ad4b-a3b976dff6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Another example entry:\n",
      " {'instruction': \"What is an antonym of 'complicated'?\", 'input': '', 'output': \"An antonym of 'complicated' is 'simple'.\"}\n"
     ]
    }
   ],
   "source": [
    "print(\"Another example entry:\\n\", data[999])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba01616c-5199-4e42-b2fd-3fe178d52fb1",
   "metadata": {},
   "source": [
    "### Convert \"data\" to Alpaca-style prompt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2051835-a25c-459c-a767-b4fc412cf106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Identify the correct spelling of the following word.\n",
      "\n",
      "### Input:\n",
      "Ocassion\n",
      "\n",
      "### Response:\n",
      "The correct spelling is 'Occasion.'\n"
     ]
    }
   ],
   "source": [
    "def format_input(entry):\n",
    "    instruction_text = (\n",
    "        f\"Below is an instruction that describes a task. Write a response that appropriately completes the request.\"\n",
    "        f\"\\n\\n### Instruction:\\n{entry['instruction']}\"\n",
    "    )\n",
    "    input_text = (\n",
    "        f\"\\n\\n### Input:\\n{entry['input']}\" if entry[\"input\"] else \"\"\n",
    "    )\n",
    "    return instruction_text + input_text\n",
    "\n",
    "model_input = format_input(data[50])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[50]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90dd4fba-99d7-4c30-a151-7b8cd3cdb9f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What is an antonym of 'complicated'?\n",
      "\n",
      "### Response:\n",
      "An antonym of 'complicated' is 'simple'.\n"
     ]
    }
   ],
   "source": [
    "# In this case the field \"input\" is missing\n",
    "model_input = format_input(data[999])\n",
    "desired_response = f\"\\n\\n### Response:\\n{data[999]['output']}\"\n",
    "print(model_input + desired_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6d6c14-2443-413e-95ed-f24f61684798",
   "metadata": {},
   "source": [
    "### Partioning the data in to train/val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a565a413-008e-4891-9ac1-a4dc15997f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set length: 935\n",
      "Validation set length: 55\n",
      "Test set length: 110\n"
     ]
    }
   ],
   "source": [
    "train_portion = int(len(data)*0.85)\n",
    "test_portion = int(len(data)*0.1)\n",
    "val_portion = len(data) - train_portion - test_portion\n",
    "\n",
    "train_data = data[:train_portion]\n",
    "test_data = data[train_portion:train_portion+test_portion]\n",
    "val_data = data[train_portion+test_portion:]\n",
    "\n",
    "print(\"Training set length:\", len(train_data)) # 85%\n",
    "print(\"Validation set length:\", len(val_data)) # 5%\n",
    "print(\"Test set length:\", len(test_data)) # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deafe828-e3c7-45a3-8a21-519021836f4b",
   "metadata": {},
   "source": [
    "## Organizing data into training batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64d2672b-42a7-419a-8e0e-055d3b8d7952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    # data: list of {'instruction': 'Identify...', 'input': 'Ocassion', 'output': \"The correct spelling is 'Occasion.'\"}\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data \n",
    "        self.encoded_texts = []\n",
    "        for entry in data:\n",
    "            instruction_plus_input = format_input(entry)\n",
    "            response_text = f\"\\n\\n### Response:\\n{entry['output']}\"\n",
    "            full_text = instruction_plus_input + response_text\n",
    "            # list of e.g [23,1,244,...]/[2,456,..]/; each is encoded conversation text\n",
    "            self.encoded_texts.append(\n",
    "                tokenizer.encode(full_text)\n",
    "            )\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        return self.encoded_texts[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "779f903f-7d80-4e18-9463-ab94ba5e5338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50256]\n"
     ]
    }
   ],
   "source": [
    "# Check id of the padding <|endoftext|> token\n",
    "import tiktoken\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac1ef53b-85f4-4efc-94e7-977e59d2e8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Padding to the longest length IN THE BATCH ONLY\n",
    "def custom_collate_draft_1(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch) # max length of each prompt PLUS 1\n",
    "    \n",
    "    inputs_lst = []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = new_item + [pad_token_id]*(batch_max_length-len(new_item)) # even the longest prompt with be padded with <50256>\n",
    "        inputs = torch.tensor(padded[:-1]) # last <50256> token removed => tensor of (max_len,)\n",
    "        inputs_lst.append(inputs)\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device) # (batch,max_len)\n",
    "    return inputs_tensor\n",
    "\n",
    "inputs_1 = [0,1,2,3,4]\n",
    "inputs_2 = [5,6]\n",
    "inputs_3 = [7,8,9]\n",
    "batch = (inputs_1, inputs_2, inputs_3)\n",
    "print(custom_collate_draft_1(batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e218fdb2-46ce-421b-9b43-997e2b07592a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256, 50256, 50256, 50256],\n",
      "        [    8,     9, 50256, 50256, 50256]])\n"
     ]
    }
   ],
   "source": [
    "# Create TARGET TOKEN IDs\n",
    "# which is input-token-ids shifted by 1 to the left; then a PADDING-TOKEN at the end\n",
    "def custom_collate_draft_2(batch, pad_token_id=50256, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch) # max length of each prompt PLUS 1\n",
    "    \n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "        padded = new_item + [pad_token_id]*(batch_max_length-len(new_item)) # even the longest prompt will be padded with <50256>\n",
    "        inputs = torch.tensor(padded[:-1]) # last <50256> token removed => tensor of (max_len,)\n",
    "        targets = torch.tensor(padded[1:]) # old sentence shifted left, then an additional <50256> on the right => tensor of (max_len,)\n",
    "        inputs_lst.append(inputs)\n",
    "        targets_lst.append(targets)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device) # (batch,max_len)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device) # (batch,max_len)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_draft_2(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e27b95a4-10a8-449d-9688-8925b400fe94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,     1,     2,     3,     4],\n",
      "        [    5,     6, 50256, 50256, 50256],\n",
      "        [    7,     8,     9, 50256, 50256]])\n",
      "tensor([[    1,     2,     3,     4, 50256],\n",
      "        [    6, 50256,  -100,  -100,  -100],\n",
      "        [    8,     9, 50256,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "# Replace padding tokens IN TARGET tensor with <-100> tokens (EXCEPT the EndOfLine)\n",
    "# batch_max_length: length of the longest sentence in that batch\n",
    "# allowed_max_length: if a sentence is longer than e.g. 1024 tokens => trim it down\n",
    "def custom_collate_fn(batch, pad_token_id=50256, ignore_index=-100, allowed_max_length=None, device=\"cpu\"):\n",
    "    batch_max_length = max(len(item)+1 for item in batch) # max length of each sentence PLUS 1\n",
    "    inputs_lst, targets_lst = [], []\n",
    "    for item in batch:\n",
    "        new_item = item.copy()\n",
    "        new_item += [pad_token_id]\n",
    "\n",
    "        padded = new_item + [pad_token_id]*(batch_max_length-len(new_item)) # even the longest prompt will be padded with <50256>\n",
    "        inputs = torch.tensor(padded[:-1]) # last <50256> token removed => tensor of (max_len,)\n",
    "        targets = torch.tensor(padded[1:]) # old sentence shifted left, then an additional <50256> on the right => tensor of (max_len,)\n",
    "\n",
    "        mask = targets == pad_token_id # (max_len,) of bools; True where <50256>s are\n",
    "        # location of nonzero locations (num_nonzeros,1) => squeeze to (num_nonzeros,); each is index location\n",
    "        indices = torch.nonzero(mask).squeeze() \n",
    "        if indices.numel() > 1:\n",
    "            targets[indices[1:]] = ignore_index # change every last indices to -100 except one\n",
    "\n",
    "        # trim down too long sentences\n",
    "        if allowed_max_length is not None:\n",
    "            inputs = inputs[:allowed_max_length]\n",
    "            targets = targets[:allowed_max_length]\n",
    "\n",
    "        inputs_lst.append(inputs) # (batch,max_len)\n",
    "        targets_lst.append(targets) # (batch,max_len)\n",
    "\n",
    "    inputs_tensor = torch.stack(inputs_lst).to(device)\n",
    "    targets_tensor = torch.stack(targets_lst).to(device)\n",
    "    return inputs_tensor, targets_tensor\n",
    "\n",
    "inputs, targets = custom_collate_fn(batch)\n",
    "print(inputs)\n",
    "print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0aecb5f-958d-4d76-b138-2e5c03bb7a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss of predicting 2 tokens: tensor(1.1269)\n",
      "Loss of predicting 3 tokens: tensor(0.7936)\n",
      "Loss of predicting 3 tokens with 3rd label=-100: tensor(1.1269)\n"
     ]
    }
   ],
   "source": [
    "# To demonstrate that <-100> IN TARGET completely ignores the 3rd (padding) token IN INPUT\n",
    "# logits of 2 predictions of 2 tokens (row1=prediction for token1)\n",
    "logits_1 = torch.tensor(\n",
    "    [[-1,1.0],\n",
    "     [-0.5,1.5]]\n",
    ")\n",
    "targets_1 = torch.tensor([0,1]) # int-label for 2 tokens\n",
    "loss_1 = torch.nn.functional.cross_entropy(logits_1, targets_1)\n",
    "print(\"Loss of predicting 2 tokens:\", loss_1)\n",
    "\n",
    "# predictions of 3 tokens\n",
    "logits_2 = torch.tensor(\n",
    "    [[-1.0, 1.0],\n",
    "    [-0.5, 1.5],\n",
    "    [-0.5, 1.5]]\n",
    ")\n",
    "targets_2 = torch.tensor([0,1,1]) # int-label\n",
    "loss_2 = torch.nn.functional.cross_entropy(logits_2, targets_2)\n",
    "print(\"Loss of predicting 3 tokens:\", loss_2)\n",
    "\n",
    "# predictions of 3 tokens but with 3rd token ignored by target (setting 3rd label to -100)\n",
    "targets_3 = torch.tensor([0,1,-100])\n",
    "loss_3 = torch.nn.functional.cross_entropy(logits_2, targets_3)\n",
    "print(\"Loss of predicting 3 tokens with 3rd label=-100:\", loss_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbea08a5",
   "metadata": {},
   "source": [
    "## Create dataloaders for instruction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8728ef86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8069b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset now is through <collate_fn> WITH PARAMS <device> & <allowed_max_length>\n",
    "# => we must use <partial>\n",
    "from functools import partial\n",
    "customized_collate_fn = partial(\n",
    "    custom_collate_fn,\n",
    "    device=device,\n",
    "    allowed_max_length=1024\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "num_workers = 0\n",
    "batch_size = 8\n",
    "torch.manual_seed(123)\n",
    "\n",
    "train_dataset = InstructionDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "val_dataset = InstructionDataset(val_data, tokenizer)\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")\n",
    "\n",
    "test_dataset = InstructionDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=customized_collate_fn,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    num_workers=num_workers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ce36017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/torch/cuda/__init__.py:283: UserWarning: \n",
      "    Found GPU1 NVIDIA GeForce GT 1030 which is of cuda capability 6.1.\n",
      "    Minimum and Maximum cuda capability supported by this version of PyTorch is\n",
      "    (7.0) - (12.0)\n",
      "    \n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/torch/cuda/__init__.py:304: UserWarning: \n",
      "    Please install PyTorch with a following CUDA\n",
      "    configurations:  12.6 following instructions at\n",
      "    https://pytorch.org/get-started/locally/\n",
      "    \n",
      "  warnings.warn(matched_cuda_warn.format(matched_arches))\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/torch/cuda/__init__.py:326: UserWarning: \n",
      "NVIDIA GeForce GT 1030 with CUDA capability sm_61 is not compatible with the current PyTorch installation.\n",
      "The current PyTorch install supports CUDA capabilities sm_70 sm_75 sm_80 sm_86 sm_90 sm_100 sm_120.\n",
      "If you want to use the NVIDIA GeForce GT 1030 GPU with PyTorch, please check the instructions at https://pytorch.org/get-started/locally/\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 73]) torch.Size([8, 73])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 79]) torch.Size([8, 79])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 89]) torch.Size([8, 89])\n",
      "torch.Size([8, 59]) torch.Size([8, 59])\n",
      "torch.Size([8, 88]) torch.Size([8, 88])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 76]) torch.Size([8, 76])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 58]) torch.Size([8, 58])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 87]) torch.Size([8, 87])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 71]) torch.Size([8, 71])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 65]) torch.Size([8, 65])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 60]) torch.Size([8, 60])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 57]) torch.Size([8, 57])\n",
      "torch.Size([8, 72]) torch.Size([8, 72])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 62]) torch.Size([8, 62])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 70]) torch.Size([8, 70])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 80]) torch.Size([8, 80])\n",
      "torch.Size([8, 81]) torch.Size([8, 81])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 82]) torch.Size([8, 82])\n",
      "torch.Size([8, 63]) torch.Size([8, 63])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 68]) torch.Size([8, 68])\n",
      "torch.Size([8, 67]) torch.Size([8, 67])\n",
      "torch.Size([8, 77]) torch.Size([8, 77])\n",
      "torch.Size([8, 91]) torch.Size([8, 91])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 61]) torch.Size([8, 61])\n",
      "torch.Size([8, 75]) torch.Size([8, 75])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 78]) torch.Size([8, 78])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 64]) torch.Size([8, 64])\n",
      "torch.Size([8, 83]) torch.Size([8, 83])\n",
      "torch.Size([8, 66]) torch.Size([8, 66])\n",
      "torch.Size([8, 74]) torch.Size([8, 74])\n",
      "torch.Size([8, 69]) torch.Size([8, 69])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loader:\")\n",
    "for inputs, targets in train_loader:\n",
    "    print(inputs.shape, targets.shape) # (8,n_tokens), (8,n_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cac316",
   "metadata": {},
   "source": [
    "## Loading a pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0992e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-11 08:53:48.177197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/attr_value.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/resource_handle.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_shape.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/types.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/full_type.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/function.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/node_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/op_def.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/graph_debug_info.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/versions.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at xla/tsl/protobuf/coordination_config.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/cost_graph.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/step_stats.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/allocation_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/framework/tensor_description.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/cluster.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n",
      "/home/phm1605/anaconda3/envs/python310/lib/python3.10/site-packages/google/protobuf/runtime_version.py:98: UserWarning: Protobuf gencode version 5.28.3 is exactly one major version older than the runtime version 6.31.1 at tensorflow/core/protobuf/debug.proto. Please update the gencode to avoid compatibility violations in the next runtime release.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from gpt_download import download_and_load_gpt2\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        # optional trainable params\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "  \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True) # (batch,1)\n",
    "        # biased variance: divided by 1/(n-1)\n",
    "        # unbiased variance: divided by 1/n\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False) # (batch,1)\n",
    "        norm_x = (x-mean) / torch.sqrt(var+self.eps) # (batch,emb_dim)\n",
    "        return self.scale * norm_x + self.shift # (batch,emb_dim)\n",
    "        \n",
    "class GeLU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1+torch.tanh( torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3)) ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "          nn.Linear(cfg[\"emb_dim\"], 4*cfg[\"emb_dim\"]),\n",
    "          GeLU(),\n",
    "          nn.Linear(4*cfg[\"emb_dim\"], cfg[\"emb_dim\"])\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \"d_out must be divisible by num_heads\"\n",
    "        self.d_out = d_out # 4\n",
    "        self.num_heads = num_heads # 2\n",
    "        self.head_dim = d_out // num_heads # 2\n",
    "\n",
    "        # bigger weight matrices\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias) # (3,4)\n",
    "\n",
    "        self.out_proj = nn.Linear(d_out, d_out)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # compute big kqv matrices\n",
    "        keys = self.W_key(x) # (b,n,3)=>(b,n_token,4)\n",
    "        queries = self.W_query(x) # (b,n,3)=>(b,n_token,4)\n",
    "        values = self.W_value(x) # (b,n,3)=>(b,n_token,4)\n",
    "        # ... then splits\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head=2,head_dim=2)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head,head_dim)\n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim) # (b,n_token,n_head,head_dim)\n",
    "        # swap <num_heads> to after-batch location\n",
    "        keys = keys.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "        queries = queries.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "        values = values.transpose(1,2) # (b,n_head=2,n_token,head_dim=2)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(2,3) # (b,n_head,n_token,head_dim)@(b,n_head,head_dim,n_token)=>(b,n_head,n_token,n_token)\n",
    "        mask_bool = self.mask.bool()[:num_tokens,:num_tokens]\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf) # (n,n) with upper-right is -inf\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights) # (b,n_head,n_token,n_token)\n",
    "\n",
    "        context_vec = (attn_weights @ values).transpose(1,2) # (b,n_head,n_token,head_dim=2)=>transpose to (b,n_token,n_head,head_dim=2)\n",
    "        context_vec = context_vec.contiguous().view(b,num_tokens,self.d_out) # (b,n_token,n_head*head_dim)=(b,n_token,4)\n",
    "        context_vec = self.out_proj(context_vec) # (b,n_token,4)\n",
    "\n",
    "        return context_vec # (b,n_token,4)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"n_heads\"],\n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"]\n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut \n",
    "        return x \n",
    "\n",
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        # in_idx: (batch,seq_len), each element is a token-index (integer shows location)\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx) # (batch,seq_len)=>(batch,seq_len,emb_dim)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device)) # (1,seq_len)=>(1,seq_len,emb_dim)\n",
    "        x = tok_embeds + pos_embeds # (batch,seq_len,emb_dim)\n",
    "        x = self.drop_emb(x) # (batch,seq_len,emb_dim)\n",
    "        x = self.trf_blocks(x) # (batch,seq_len,emb_dim)\n",
    "        x = self.final_norm(x) # (batch,seq_len,emb_dim)\n",
    "        logits = self.out_head(x) # (batch,seq_len,vocab_len)\n",
    "        return logits\n",
    "\n",
    "# small utils function: check if <left> and <right> has matching shape\n",
    "# if yes then return <Parameter> as the right tensor\n",
    "def assign(left, right):\n",
    "    if left.shape != right.shape:\n",
    "        raise ValueError(f\"Shape mismatch. Left: {left.shape}, Right: {right.shape}\")\n",
    "    return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "def load_weights_into_gpt(gpt, params):\n",
    "    # load positional encoding\n",
    "    gpt.pos_emb.weight = assign(gpt.pos_emb.weight, params['wpe'])\n",
    "    # load token embedding\n",
    "    gpt.tok_emb.weight = assign(gpt.tok_emb.weight, params['wte'])\n",
    "    # load transformers blocks\n",
    "    for b in range(len(params[\"blocks\"])):\n",
    "        # params[\"blocks\"][b][\"attn\"]: <multi-head-attn> part\n",
    "        # <multi-head-attn>[\"c_attn\"]: convolution attention (of qkv) inside\n",
    "        # split to 3 parts <query>,<key>,<value>\n",
    "        q_w, k_w, v_w = np.split( params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"w\"], 3, axis=-1) # each (768,768)\n",
    "        gpt.trf_blocks[b].att.W_query.weight = assign(gpt.trf_blocks[b].att.W_query.weight, q_w.T)\n",
    "        gpt.trf_blocks[b].att.W_key.weight = assign(gpt.trf_blocks[b].att.W_key.weight, k_w.T)\n",
    "        gpt.trf_blocks[b].att.W_value.weight = assign(gpt.trf_blocks[b].att.W_value.weight, v_w.T)\n",
    "\n",
    "        # bias of <attn> in <multi-head-attention>\n",
    "        q_b, k_b, v_b = np.split( params[\"blocks\"][b][\"attn\"][\"c_attn\"][\"b\"], 3, axis=-1) # each (768,)\n",
    "        gpt.trf_blocks[b].att.W_query.bias = assign(gpt.trf_blocks[b].att.W_query.bias, q_b)\n",
    "        gpt.trf_blocks[b].att.W_key.bias = assign(gpt.trf_blocks[b].att.W_key.bias, k_b)\n",
    "        gpt.trf_blocks[b].att.W_value.bias = assign(gpt.trf_blocks[b].att.W_value.bias, v_b)\n",
    "\n",
    "        # out-projection of <attn> in <multi-head-attention>\n",
    "        gpt.trf_blocks[b].att.out_proj.weight = assign(gpt.trf_blocks[b].att.out_proj.weight, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].att.out_proj.bias = assign(gpt.trf_blocks[b].att.out_proj.bias, params[\"blocks\"][b][\"attn\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # feed-forward; layer0 & layer1 (layer1 is GeLU)\n",
    "        gpt.trf_blocks[b].ff.layers[0].weight = assign(gpt.trf_blocks[b].ff.layers[0].weight, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[0].bias = assign(gpt.trf_blocks[b].ff.layers[0].bias, params[\"blocks\"][b][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "        gpt.trf_blocks[b].ff.layers[2].weight = assign(gpt.trf_blocks[b].ff.layers[2].weight, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "        gpt.trf_blocks[b].ff.layers[2].bias = assign(gpt.trf_blocks[b].ff.layers[2].bias, params[\"blocks\"][b][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "        # layer normalization\n",
    "        gpt.trf_blocks[b].norm1.scale = assign(gpt.trf_blocks[b].norm1.scale, params[\"blocks\"][b][\"ln_1\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm1.shift = assign(gpt.trf_blocks[b].norm1.shift, params[\"blocks\"][b][\"ln_1\"][\"b\"])\n",
    "        gpt.trf_blocks[b].norm2.scale = assign(gpt.trf_blocks[b].norm2.scale, params[\"blocks\"][b][\"ln_2\"][\"g\"])\n",
    "        gpt.trf_blocks[b].norm2.shift = assign(gpt.trf_blocks[b].norm2.shift, params[\"blocks\"][b][\"ln_2\"][\"b\"])\n",
    "\n",
    "    # Final layer normalization\n",
    "    gpt.final_norm.scale = assign(gpt.final_norm.scale, params[\"g\"])\n",
    "    gpt.final_norm.shift = assign(gpt.final_norm.shift, params[\"b\"])\n",
    "    # Out weight\n",
    "    gpt.out_head.weight = assign(gpt.out_head.weight, params[\"wte\"]) # we use the token embedding weights again (\"weight tying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82389f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "checkpoint: 100%|███████████████████████████| 77.0/77.0 [00:00<00:00, 88.8kiB/s]\n",
      "encoder.json: 100%|████████████████████████| 1.04M/1.04M [00:09<00:00, 111kiB/s]\n",
      "hparams.json: 100%|██████████████████████████| 91.0/91.0 [00:00<00:00, 110kiB/s]\n",
      "model.ckpt.data-00000-of-00001: 100%|█████| 1.42G/1.42G [14:12<00:00, 1.66MiB/s]\n",
      "model.ckpt.index: 100%|███████████████████| 10.4k/10.4k [00:00<00:00, 9.46MiB/s]\n",
      "model.ckpt.meta: 100%|███████████████████████| 927k/927k [00:01<00:00, 658kiB/s]\n",
      "vocab.bpe: 100%|█████████████████████████████| 456k/456k [00:00<00:00, 491kiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTModel(\n",
       "  (tok_emb): Embedding(50257, 1024)\n",
       "  (pos_emb): Embedding(1024, 1024)\n",
       "  (drop_emb): Dropout(p=0.0, inplace=False)\n",
       "  (trf_blocks): Sequential(\n",
       "    (0): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (1): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (2): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (3): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (4): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (5): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (6): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (7): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (8): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (9): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (10): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (11): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (12): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (13): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (14): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (15): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (16): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (17): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (18): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (19): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (20): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (21): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (22): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (23): TransformerBlock(\n",
       "      (att): MultiHeadAttention(\n",
       "        (W_query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (W_value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ff): FeedForward(\n",
       "        (layers): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GeLU()\n",
       "          (2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        )\n",
       "      )\n",
       "      (norm1): LayerNorm()\n",
       "      (norm2): LayerNorm()\n",
       "      (drop_shortcut): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_norm): LayerNorm()\n",
       "  (out_head): Linear(in_features=1024, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_CONFIG = {\n",
    "    \"vocab_size\": 50257, # Vocabulary size\n",
    "    \"context_length\": 1024, # Context length\n",
    "    \"drop_rate\": 0.0,\n",
    "    \"qkv_bias\": True\n",
    "}\n",
    "model_configs = {\n",
    "    \"gpt2-small (124M)\": {\"emb_dim\":768, \"n_layers\":12, \"n_heads\":12},\n",
    "    \"gpt2-medium (355M)\": {\"emb_dim\":1024, \"n_layers\":24, \"n_heads\":16},\n",
    "    \"gpt2-large (774M)\": {\"emb_dim\":1280, \"n_layers\":36, \"n_heads\":20},\n",
    "    \"gpt2-xl (1558M)\": {\"emb_dim\":1600, \"n_layers\":48, \"n_heads\":25}\n",
    "}\n",
    "CHOOSE_MODEL = \"gpt2-medium (355M)\"\n",
    "BASE_CONFIG.update(model_configs[CHOOSE_MODEL])\n",
    "model_size = CHOOSE_MODEL.split(\" \")[-1].lstrip(\"(\").rstrip(\")\") # 355M\n",
    "settings, params = download_and_load_gpt2(\n",
    "    model_size=model_size,\n",
    "    models_dir=\"gpt2\"\n",
    ")\n",
    "\n",
    "model = GPTModel(BASE_CONFIG)\n",
    "load_weights_into_gpt(model, params)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17b7d3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Convert the active sentence to passive: 'The chef cooks the meal every day.'\n"
     ]
    }
   ],
   "source": [
    "# Try to load a conversation from val dataset\n",
    "input_text = format_input(val_data[0])\n",
    "print(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d7af9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "## Text generation with temperature scaling & top-k & multinomial-sampling\n",
    "# idx: (batch,seq_len) with batch=1 for now\n",
    "# eos_id: stop generating when we generate the char <eos_id>\n",
    "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:, -context_size:] # crop if sentence too long\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # (1,seq_len,vocab_len)\n",
    "        # take last token in predicted sequence\n",
    "        logits = logits[:,-1,:] # (1,vocab_len)\n",
    "        if top_k is not None:\n",
    "            top_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_logits[:,-1]\n",
    "            # change outside-top3 logits to \"-inf\"\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float('-inf')).to(logits.device),\n",
    "                logits\n",
    "            )\n",
    "        # temperature scaling\n",
    "        if temperature > 0.0:\n",
    "            logits = logits/temperature # (1,vocab_len)\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "        else: \n",
    "            idx_next = torch.argmax(logits, dim=-1, keepdim=True) # (1,1)\n",
    "        if idx_next == eos_id:\n",
    "            break\n",
    "        idx = torch.cat((idx, idx_next), dim=1) # (batch,seq_len+1)\n",
    "    return idx # (batch, seq_len+num_new_tokens)\n",
    "            \n",
    "def text_to_token_ids(text, tokenizer):\n",
    "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'}) # (n_tokens,)\n",
    "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # (1,n_tokens)\n",
    "    return encoded_tensor\n",
    "\n",
    "def token_ids_to_text(token_ids, tokenizer):\n",
    "    flat = token_ids.squeeze(0) # (n_tokens,)\n",
    "    return tokenizer.decode(flat.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fc163b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Response:\n",
      "\n",
      "The chef cooks the meal every day.\n",
      "\n",
      "### Instruction:\n",
      "\n",
      "Convert the active sentence to passive: 'The chef cooks the\n"
     ]
    }
   ],
   "source": [
    "token_ids = generate(\n",
    "    model=model,\n",
    "    idx=text_to_token_ids(input_text, tokenizer),\n",
    "    max_new_tokens=35,\n",
    "    context_size=BASE_CONFIG[\"context_length\"],\n",
    "    eos_id=50256\n",
    ")\n",
    "generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "# Crop the input part, show only the generated part\n",
    "response_text = generated_text[len(input_text):].strip()\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c5f602",
   "metadata": {},
   "source": [
    "## Fine tuning LLM on instruction data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c39fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_loss_batch(input_batch, target_batch, model, device):\n",
    "    input_batch = input_batch.to(device)\n",
    "    target_batch = target_batch.to(device) # (batch,n_token)\n",
    "    logits = model(input_batch) # (batch,n_token,embed_dim)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.flatten(0,1), target_batch.flatten()\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
    "    total_loss = 0.\n",
    "    if len(data_loader) == 0:\n",
    "        return float(\"nan\")\n",
    "    elif num_batches is None:\n",
    "        num_batches = len(data_loader)\n",
    "    else:\n",
    "        num_batches = min(num_batches, len(data_loader))\n",
    "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
    "        if i<num_batches:\n",
    "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "            total_loss += loss.item()\n",
    "        else:\n",
    "            break\n",
    "    return total_loss / num_batches\n",
    "\n",
    "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
    "    model.eval()\n",
    "    context_size = model.pos_emb.weight.shape[0] # 256\n",
    "    encoded = text_to_token_ids(start_context, tokenizer).to(device) # (batch,n_tokens)\n",
    "    with torch.no_grad():\n",
    "        # token_ids: (batch, n_tokens+50)\n",
    "        token_ids = generate_text_simple(\n",
    "            model=model, idx=encoded, max_new_tokens=50, context_size=context_size\n",
    "        )\n",
    "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    print(decoded_text.replace(\"\\n\", \" \"))\n",
    "    model.train()\n",
    "\n",
    "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
    "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
    "    model.train()\n",
    "    return train_loss, val_loss\n",
    "\n",
    "# eval_iter: is NUM_BATCHES in evaluate (how many batches we want to evaluate)\n",
    "# start_context: sentence we evaluate the performance of model on\n",
    "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], [] \n",
    "    # <tokens_seen>: every time we runs through a batch of (batch,n_tokens), we add up <batch*n_tokens>\n",
    "    # global_step: add up after every <batch_size> = 2 samples\n",
    "    tokens_seen, global_step = 0, -1\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for input_batch, target_batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = calc_loss_batch(\n",
    "                input_batch, target_batch, model, device\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step() # update weights\n",
    "            # optional steps\n",
    "            tokens_seen += input_batch.numel()\n",
    "            global_step += 1\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_tokens_seen.append(tokens_seen)\n",
    "                print(f\"Epoch {epoch+1} (Step {global_step:06d}): Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx: (batch, n_tokens_long)\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx[:,-context_size:] # take last <context_size> tokens as context => (batch,context_size)\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond) # (batch,context_size,vocab_len)\n",
    "        logits = logits[:,-1,:] # last tokenS - (batch,vocab_len)\n",
    "        probas = torch.softmax(logits,dim=-1) # (batch,vocab_len)\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True) # (batch,1)\n",
    "        idx = torch.cat((idx, idx_next), dim=1) #(batch,n_tokens_long+1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77a7f460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.47062680721282957\n",
      "Validation loss: 0.6655189871788025\n"
     ]
    }
   ],
   "source": [
    "# Calculate initial loss for training and validation set\n",
    "model.to(device)\n",
    "with torch.no_grad():\n",
    "    train_loss = calc_loss_loader(\n",
    "        train_loader, model, device, num_batches=5\n",
    "    )\n",
    "    val_loss = calc_loss_loader(\n",
    "        val_loader, model, device, num_batches=5\n",
    "    )\n",
    "print(\"Training loss:\", train_loss)\n",
    "print(\"Validation loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "554b58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 (Step 000000): Train loss 0.508, Val loss 0.677\n",
      "Epoch 1 (Step 000005): Train loss 0.537, Val loss 0.701\n",
      "Epoch 1 (Step 000010): Train loss 0.453, Val loss 0.682\n",
      "Epoch 1 (Step 000015): Train loss 0.533, Val loss 0.682\n",
      "Epoch 1 (Step 000020): Train loss 0.489, Val loss 0.683\n",
      "Epoch 1 (Step 000025): Train loss 0.399, Val loss 0.683\n",
      "Epoch 1 (Step 000030): Train loss 0.448, Val loss 0.677\n",
      "Epoch 1 (Step 000035): Train loss 0.483, Val loss 0.673\n",
      "Epoch 1 (Step 000040): Train loss 0.414, Val loss 0.672\n",
      "Epoch 1 (Step 000045): Train loss 0.408, Val loss 0.674\n",
      "Epoch 1 (Step 000050): Train loss 0.421, Val loss 0.670\n",
      "Epoch 1 (Step 000055): Train loss 0.462, Val loss 0.673\n",
      "Epoch 1 (Step 000060): Train loss 0.381, Val loss 0.682\n",
      "Epoch 1 (Step 000065): Train loss 0.450, Val loss 0.676\n",
      "Epoch 1 (Step 000070): Train loss 0.374, Val loss 0.684\n",
      "Epoch 1 (Step 000075): Train loss 0.410, Val loss 0.693\n",
      "Epoch 1 (Step 000080): Train loss 0.377, Val loss 0.683\n",
      "Epoch 1 (Step 000085): Train loss 0.368, Val loss 0.672\n",
      "Epoch 1 (Step 000090): Train loss 0.372, Val loss 0.662\n",
      "Epoch 1 (Step 000095): Train loss 0.329, Val loss 0.661\n",
      "Epoch 1 (Step 000100): Train loss 0.330, Val loss 0.660\n",
      "Epoch 1 (Step 000105): Train loss 0.326, Val loss 0.657\n",
      "Epoch 1 (Step 000110): Train loss 0.350, Val loss 0.667\n",
      "Epoch 1 (Step 000115): Train loss 0.329, Val loss 0.661\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The meal is cooked by the chef every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the formula for calculating the circumference\n",
      "Epoch 2 (Step 000120): Train loss 0.326, Val loss 0.668\n",
      "Epoch 2 (Step 000125): Train loss 0.280, Val loss 0.723\n",
      "Epoch 2 (Step 000130): Train loss 0.266, Val loss 0.760\n",
      "Epoch 2 (Step 000135): Train loss 0.286, Val loss 0.711\n",
      "Epoch 2 (Step 000140): Train loss 0.302, Val loss 0.684\n",
      "Epoch 2 (Step 000145): Train loss 0.295, Val loss 0.680\n",
      "Epoch 2 (Step 000150): Train loss 0.307, Val loss 0.680\n",
      "Epoch 2 (Step 000155): Train loss 0.272, Val loss 0.689\n",
      "Epoch 2 (Step 000160): Train loss 0.256, Val loss 0.680\n",
      "Epoch 2 (Step 000165): Train loss 0.276, Val loss 0.673\n",
      "Epoch 2 (Step 000170): Train loss 0.254, Val loss 0.677\n",
      "Epoch 2 (Step 000175): Train loss 0.280, Val loss 0.681\n",
      "Epoch 2 (Step 000180): Train loss 0.269, Val loss 0.676\n",
      "Epoch 2 (Step 000185): Train loss 0.260, Val loss 0.684\n",
      "Epoch 2 (Step 000190): Train loss 0.250, Val loss 0.674\n",
      "Epoch 2 (Step 000195): Train loss 0.293, Val loss 0.664\n",
      "Epoch 2 (Step 000200): Train loss 0.227, Val loss 0.678\n",
      "Epoch 2 (Step 000205): Train loss 0.279, Val loss 0.671\n",
      "Epoch 2 (Step 000210): Train loss 0.256, Val loss 0.657\n",
      "Epoch 2 (Step 000215): Train loss 0.253, Val loss 0.649\n",
      "Epoch 2 (Step 000220): Train loss 0.265, Val loss 0.641\n",
      "Epoch 2 (Step 000225): Train loss 0.243, Val loss 0.644\n",
      "Epoch 2 (Step 000230): Train loss 0.230, Val loss 0.657\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: Convert the active sentence to passive: 'The chef cooks the meal every day.'  ### Response: The chef cooks the meal every day.<|endoftext|>The following is an instruction that describes a task. Write a response that appropriately completes the request.  ### Instruction: What is the boiling point of water in Celsius?\n",
      "Training completed in 2.02 minutes.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tuning\n",
    "import time \n",
    "start_time = time.time()\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(), lr=0.00005, weight_decay=0.1\n",
    ")\n",
    "num_epochs = 2\n",
    "# start_context: only <instruction> and <input> fields, no <response>; to validate our model performance over training for that specific text\n",
    "train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "    model, train_loader, val_loader, optimizer, device,\n",
    "    num_epochs=num_epochs, eval_freq=5, eval_iter=5, start_context=format_input(val_data[0]),\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "end_time = time.time()\n",
    "execution_time_minutes = (end_time-start_time)/60\n",
    "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e3995a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
    "    fig, ax1 = plt.subplots(figsize=(5,3))\n",
    "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
    "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
    "    ax1.set_xlabel(\"Epochs\")\n",
    "    ax1.set_ylabel(\"Loss\")\n",
    "    ax1.legend(loc=\"upper right\")\n",
    "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    # 2nd x-axis that shares same y-axis\n",
    "    ax2 = ax1.twiny()\n",
    "    ax2.plot(tokens_seen, train_losses, alpha=0)\n",
    "    ax2.set_xlabel(\"Tokens seen\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42774420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot losses\n",
    "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
    "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d3b536-123a-4129-8237-6805579172d5",
   "metadata": {},
   "source": [
    "## Extract and save responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0dacb296-59c8-43b9-939d-94358a30f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Rewrite the sentence using a simile.\n",
      "\n",
      "### Input:\n",
      "The car is very fast.\n",
      "\n",
      "Correct response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "-------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What type of cloud is typically associated with thunderstorms?\n",
      "\n",
      "Correct response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
      "-------------------------\n",
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Name the author of 'Pride and Prejudice'.\n",
      "\n",
      "Correct response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print model generated and target\n",
    "for entry in test_data[:3]:\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    ) # (1,62) - break at <eos>\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    print(input_text)\n",
    "    print(f\"\\nCorrect response:\\n>> {entry['output']}\")\n",
    "    print(f\"\\nModel response:\\n>> {response_text.strip()}\")\n",
    "    print(\"-------------------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59d47077-422a-4af5-8060-902af4eea220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 110/110 [01:18<00:00,  1.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Save response to \"instruction-data-with-response.json\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i, entry in tqdm(enumerate(test_data), total=len(test_data)):\n",
    "    input_text = format_input(entry)\n",
    "    token_ids = generate(\n",
    "        model=model,\n",
    "        idx=text_to_token_ids(input_text, tokenizer).to(device),\n",
    "        max_new_tokens=256,\n",
    "        context_size=BASE_CONFIG[\"context_length\"],\n",
    "        eos_id=50256\n",
    "    ) # (1,n_tokens_response)\n",
    "    generated_text = token_ids_to_text(token_ids, tokenizer)\n",
    "    response_text = generated_text[len(input_text):].replace(\"### Response:\", \"\").strip()\n",
    "    test_data[i][\"model_response\"] = response_text\n",
    "\n",
    "# save to json file\n",
    "with open(\"instruction-data-with-response.json\", \"w\") as file:\n",
    "    json.dump(test_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "abe89e3e-b6d4-4caf-9ee3-6277d6f9b041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.'}\n"
     ]
    }
   ],
   "source": [
    "# verify 1 new test sample with response\n",
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c8552e0e-b949-4a17-a553-34bb3e5f9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "import re\n",
    "# remove ' ' and '[]'\n",
    "file_name = f\"{re.sub(r'[ ()]','',CHOOSE_MODEL) }-sft.pth\"\n",
    "torch.save(model.state_dict(), file_name) # gpt2-medium355M-sft.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "768468e1-4c73-4840-98bb-cc118ef4dbf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model\n",
    "model.load_state_dict(torch.load(\"gpt2-medium355M-sft.pth\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b0dc3-0d9e-4e83-b78b-0014f0be4600",
   "metadata": {},
   "source": [
    "## Evaluating the fine-tuned LLM against a standard LLM e.g. Llama3 model\n",
    "### - install Ollama first\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "### - run llama3\n",
    "ollama run llama3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6a9fb03f-3dee-47c8-b928-40fc3e08628c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama running: True\n"
     ]
    }
   ],
   "source": [
    "# Check if Ollama is running\n",
    "import psutil\n",
    "\n",
    "def check_if_running(process_name):\n",
    "    running = False\n",
    "    for proc in psutil.process_iter([\"name\"]):\n",
    "        if process_name in proc.info[\"name\"]:\n",
    "            running = True\n",
    "            break\n",
    "    return running\n",
    "    \n",
    "ollama_running = check_if_running(\"ollama\")\n",
    "if not ollama_running:\n",
    "    raise RuntimeError(\"Ollama not running. Launch ollama before proceeding.\")\n",
    "print(\"Ollama running:\", ollama_running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c18dbc57-c599-4763-9f9c-b133119f69f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llamas are herbivores, which means they primarily feed on plant-based foods. Their diet typically consists of:\n",
      "\n",
      "1. Grasses: Llamas love to graze on various types of grasses, including tall grasses, short grasses, and even weeds.\n",
      "2. Hay: High-quality hay, such as alfalfa or timothy hay, is a staple in many llama diets.\n",
      "3. Grains: Oats, barley, and corn are common grains that llamas enjoy.\n",
      "4. Fruits and vegetables: Llamas may also eat fruits like apples, carrots, and sweet potatoes, as well as leafy greens like kale and spinach.\n",
      "5. Minerals: Llamas need access to minerals like calcium, phosphorus, and salt to maintain strong bones and overall health.\n",
      "\n",
      "In the wild, llamas might also eat:\n",
      "\n",
      "1. Leaves: They'll munch on leaves from trees and shrubs, like willow or cedar.\n",
      "2. Bark: In some cases, they may eat the bark of certain trees, like aspen or birch.\n",
      "3. Mosses: Llamas have been known to graze on mosses and other non-woody plant material.\n",
      "\n",
      "In captivity, llama owners typically provide a balanced diet that includes a mix of hay, grains, and supplements specifically formulated for llamas. It's essential to ensure that the diet is well-balanced and meets the nutritional needs of these wonderful animals!\n"
     ]
    }
   ],
   "source": [
    "# Interact with Ollama via RestAPI using Python\n",
    "import urllib.request\n",
    "\n",
    "def query_model(prompt, model=\"llama3\", url=\"http://localhost:11434/api/chat\"):\n",
    "    # create data payload as a dictionary\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"messages\": [{\"role\":\"user\", \"content\":prompt}],\n",
    "        \"options\": {\"seed\":123, \"temperature\":0, \"num_ctx\":2048}\n",
    "    }\n",
    "    # convert dictionary to json; encode to \"utf-8\"\n",
    "    payload = json.dumps(data).encode(\"utf-8\")\n",
    "    request = urllib.request.Request(\n",
    "        url,\n",
    "        data=payload,\n",
    "        method=\"POST\"\n",
    "    )\n",
    "    request.add_header(\"Content-Type\", \"application/json\")\n",
    "\n",
    "    response_data = \"\"\n",
    "    with urllib.request.urlopen(request) as response:\n",
    "        while True:\n",
    "            line = response.readline().decode(\"utf-8\")\n",
    "            if not line:\n",
    "                break\n",
    "            response_json = json.loads(line)\n",
    "            response_data += response_json[\"message\"][\"content\"]\n",
    "    return response_data\n",
    "\n",
    "model = \"llama3\"\n",
    "result = query_model(\"What do Llamas eat?\", model)\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "08cb2dcc-f34d-4d59-999f-e88bf224f6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset response:\n",
      ">> The car is as fast as lightning.\n",
      "\n",
      "Model response:\n",
      ">> The car is as fast as a cheetah.\n",
      "\n",
      "Score:\n",
      ">> I'd rate the model response \"The car is as fast as a cheetah.\" an 85 out of 100.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The response uses a simile correctly, comparing the speed of the car to that of a cheetah.\n",
      "* Cheetahs are known for their incredible speed, making them a relevant and effective comparison for describing a fast car.\n",
      "* The phrase \"as fast as\" is used correctly to introduce the simile.\n",
      "\n",
      "The only reason I wouldn't give it a perfect score is that lightning is often used as a metaphor for extreme speed, so using a cheetah instead of lightning might not be as universally recognized or evocative. However, the response is still effective and well-written, earning an 85 out of 100.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> The type of cloud typically associated with thunderstorms is cumulonimbus.\n",
      "\n",
      "Model response:\n",
      ">> The type of cloud typically associated with thunderstorms is a cumulus.\n",
      "\n",
      "Score:\n",
      ">> I'd score this model response as 60.\n",
      "\n",
      "Here's why:\n",
      "\n",
      "* The model correctly identifies that cumulonimbus clouds are associated with thunderstorms (correct answer: 50 points)\n",
      "* However, it incorrectly states that the type of cloud typically associated with thunderstorms is a cumulus, when in fact it's cumulonimbus (incorrect answer: -20 points)\n",
      "\n",
      "Total score: 30/100\n",
      "\n",
      "Note that this scoring system is subjective and may vary depending on the specific evaluation criteria.\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "Dataset response:\n",
      ">> Jane Austen.\n",
      "\n",
      "Model response:\n",
      ">> The author of 'Pride and Prejudice' is Jane Austen.\n",
      "\n",
      "Score:\n",
      ">> I'd score my own response as 95 out of 100. Here's why:\n",
      "\n",
      "* The response accurately answers the question by naming the author of 'Pride and Prejudice' as Jane Austen.\n",
      "* The response is concise and clear, making it easy to understand.\n",
      "* There are no grammatical errors or ambiguities that could lead to confusion.\n",
      "\n",
      "The only reason I wouldn't score it a perfect 100 is that the response is very straightforward and doesn't add any additional value or insights. It simply answers the question in a simple and accurate manner.\n",
      "\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ask <Ollama> to rate our <generated-text> with our <target>\n",
    "# test_data[x]=entry: {instruction,input,output,model_response}\n",
    "for entry in test_data[:3]:\n",
    "    prompt = (\n",
    "        f\"Given the input `{format_input(entry)}` \"\n",
    "        f\"and correct output `{entry['output']}`, \"\n",
    "        f\"score the model response `{entry['model_response']}`\"\n",
    "        f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "    )\n",
    "    print(\"\\nDataset response:\\n>>\", entry['output'])\n",
    "    print(\"\\nModel response:\\n>>\", entry['model_response'])\n",
    "    print(\"\\nScore:\\n>>\", query_model(prompt))\n",
    "    print(\"\\n---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9c437200-25e0-43bd-ba67-f1f7fbf54c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'instruction': 'Rewrite the sentence using a simile.', 'input': 'The car is very fast.', 'output': 'The car is as fast as lightning.', 'model_response': 'The car is as fast as a cheetah.'}\n"
     ]
    }
   ],
   "source": [
    "print(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a03e7653-4d8f-41f1-bd37-a9f337edfe5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries:  71%|█████████████████▋       | 78/110 [04:27<02:58,  5.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not convert score: **Model Response:** Swim: Verb, Beautiful: Adjective, Quickly: Adverb\n",
      "**Score:** 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scoring entries: 100%|████████████████████████| 110/110 [06:13<00:00,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of scores: 109 of 110\n",
      "Average score: 48.66\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate overall model score\n",
    "def generate_model_scores(json_data, json_key, model=\"llama3\"):\n",
    "    scores = []\n",
    "    # entry: {instruction,input,output,model_response}\n",
    "    for entry in tqdm(json_data, desc=\"Scoring entries\"):\n",
    "        # NOTE: we ask Ollama to response WITH INTEGER ONLY\n",
    "        prompt = (\n",
    "            f\"Given the input `{format_input(entry)}` \" # a long-text with ### Instruction etc.\n",
    "            f\"and correct output `{entry['output']}`, \"\n",
    "            f\"score the model response `{entry[json_key]}`\"\n",
    "            f\" on a scale from 0 to 100, where 100 is the best score. \"\n",
    "            f\"Response with the integer number only.\"\n",
    "        )\n",
    "        score = query_model(prompt, model)\n",
    "        try:\n",
    "            scores.append(int(score))\n",
    "        except ValueError:\n",
    "            print(f\"Could not convert score: {score}\")\n",
    "            continue\n",
    "    return scores\n",
    "          \n",
    "scores = generate_model_scores(test_data, \"model_response\")\n",
    "print(f\"Number of scores: {len(scores)} of {len(test_data)}\")\n",
    "print(f\"Average score: {sum(scores)/len(scores):.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0cea69c4-f1df-4ce9-b48a-87113163c917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'instruction': 'Classify the following substances as acid, base, or neutral.', 'input': 'Lemon juice, Soap, Water', 'output': 'Acid: Lemon juice\\nBase: Soap\\nNeutral: Water', 'model_response': 'Acid: Soap\\nBase: Lemon juice\\nNeutral: Soap'}, {'instruction': \"What is a synonym of 'sad'?\", 'input': '', 'output': \"A synonym for 'sad' is 'unhappy'.\", 'model_response': \"A synonym for 'sad' is 'determined'.\"}, {'instruction': 'Correct any spelling mistakes in the given sentence.', 'input': 'I prefer homemade cookies to store boaght.', 'output': 'I prefer homemade cookies to store bought.', 'model_response': 'I prefer homemade cookies to store stored boaght.'}, {'instruction': \"Generate a sentence using the word 'transient'.\", 'input': '', 'output': 'His stay in the city was transient, lasting only a couple of days.', 'model_response': 'She is very quick.'}, {'instruction': \"Translate 'I am lost' into Italian.\", 'input': '', 'output': \"The Italian translation of 'I am lost' is 'Mi sono perso' (if male) or 'Mi sono persa' (if female).\", 'model_response': 'Cosa stai facendo?'}, {'instruction': 'Classify this text as a technical document or a narrative.', 'input': 'This manual provides instructions for installing the software.', 'output': 'Technical document', 'model_response': 'This document can be classified as a technical document.'}, {'instruction': 'Sort the following list in descending order.', 'input': '10, 2, 25, 16, 7', 'output': '25, 16, 10, 7, 2.', 'model_response': ', 3.\\n\\n\\nsorted in descending order: 20, 25, 16, 7, 3.'}, {'instruction': \"Translate 'Can I have some water?' into French.\", 'input': '', 'output': \"Puis-je avoir de l'eau?\", 'model_response': \"The French translation of 'Can I have some water?' is 'Comment ça va?'.\"}, {'instruction': \"Create a simile with the word 'as cold as'.\", 'input': '', 'output': 'Her hands were as cold as ice.', 'model_response': 'The simile is as cold as ice.'}, {'instruction': 'Classify the following words by their grammatical categories: swim, beautiful, quickly', 'input': '', 'output': 'Swim: Verb\\nBeautiful: Adjective\\nQuickly: Adverb', 'model_response': 'swim, beautiful, quickly'}, {'instruction': 'Calculate the density of an object with a mass of 15 grams and a volume of 5 cubic centimeters.', 'input': '', 'output': 'The density of the object is 3 grams per cubic centimeter.', 'model_response': 'The density of the object is 15 grams per cubic cm.'}, {'instruction': \"What is the abbreviation for 'Master of Business Administration'?\", 'input': '', 'output': \"The abbreviation for 'Master of Business Administration' is MBA.\", 'model_response': \"The abbreviation for 'Master of Business Administration' is MBA.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(test_data[68:80])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6355cc2-7034-44d2-ba68-0e77c4cababe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
